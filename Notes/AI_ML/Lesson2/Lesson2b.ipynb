{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a336792f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "from scipy.stats import skewnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30ac278",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "\n",
    "### **Supervised Learning**\n",
    "Type of machine learning where the model learns using labelled data. Labelled data as explained in `lesson2a` is data that has both input/training example and the correct answer.\n",
    "Formally, you give the model pairs (x,y) where,\n",
    "x = input features\n",
    "y = correct target\n",
    "The mode learns a function **f(x) → y** from this.\n",
    "\n",
    "*It has two branches and they include:*\n",
    "\n",
    "#### 1. **Classification:**\n",
    "- Process where a model learns how to recognize categories or classes by pattern so the output is a category or class label like : \n",
    "    - Spam vs Non-Spam\n",
    "    - Disease Type A / B \n",
    "    - Dog vs Cat\n",
    "\n",
    "- *Eg. demonstration:*\n",
    "    Assuming our data as points scattered across a 2D space -- some points belong to Class A, some to Class B. The model studies where these points tend to cluster and then figures out a boundary that separates them. That boundary could be a straight line, a curve or even something very complex depending on the model. Once it learns this boundary, the model can look at a new point and decide:\n",
    "    “Which side of the boundary is this on?” aaaaaand the answer becomes the predicted class/category.\n",
    "\n",
    "- Do note that there’s no numerical meaning to the classes. Class A isn’t “higher” or “lower” than Class B - they’re just different categories. The model simply learns patterns that distinguish one category from another.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23735ba3",
   "metadata": {},
   "source": [
    "#### 2. **Regression:**\n",
    "- Process where a model learns the mathematical relationship between input features(x) and a continuous target value(y) to output a numerical value. Unlike classification where it decides which class something belongs to, the model estimates how much or how many. Used in cases like:\n",
    "    - Predicting the price of a house \n",
    "    - Estimating a student's mark this semester\n",
    "    - Predicting temperature and weather on a particular day\n",
    "\n",
    "- *Eg. demonstation:*\n",
    "    Imagine all your data points plotted on a graph where the x-axis represents the input (like square footage of a house) and the y-axis represents the target (like house price). These points won’t form a perfect straight line but they’ll show a general upward or downward pattern.\n",
    "\n",
    "    The model tries to capture that pattern by drawing the “best-fit line” (or curve) through the cloud of data points.\n",
    "    This line represents what the model has learned:\n",
    "    “As x increases or decreases, this is how y usually behaves.”\n",
    "\n",
    "    Now when the model gets a new input, it simply checks:\n",
    "    “Where would this new x fall on the learned line?”\n",
    "    The corresponding y-value becomes the predicted output.\n",
    "\n",
    "- Unlike classification, there is no concept of “boundaries” here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e0ddda",
   "metadata": {},
   "source": [
    "---\n",
<<<<<<< HEAD
    "### Machine Learning Workflow\n",
    "As discussed before, a machine learning project works in an iterative cycle. \n",
    "It is iterative because we can't possibly get everything perfect the first time. In order to better and perfect it, we need many iterations which teach us something new about the data and the problem. Here's a rough cycle of it,\n",
    "\n",
    "You look at data → clean it → build a model → evaluate it → realise something can be improved → go back → fix → try again.\n",
    "\n",
    "Let's dive into each of the steps of each iteration:\n",
    "\n",
    "#### 1. **Exploratory Data Analysis:**\n",
    "EDA is a method of analyzing datasets to summarize their main characteristics, often using visual methods. \n",
    "Here we use our raw data and question about it to obtain insights regarding the patterns in the data...but first, we split the data into \"seen\" and \"unseen\" data to make sure our insights are based on reviewing our \"seen\" sample data only.\n",
    "\n",
    "In summary this is what we explore:\n",
    "\n",
    "    - **Structure of the dataset**\n",
    "        - How many records/rows do we have? (if too few then model may not learn well)\n",
    "        - How many features/columns? (5 features will use models like Linear Regression, Logistic Regression etc. while 500 features will use models like Random Forest, XGBoost..)\n",
    "        - What are the types of values are inside those features? (Numerical? Categorical? Text?) \n",
    "\n",
    "    - **Ranges and distributions**\n",
    "        - Are numeric features spread widely?\n",
    "        - Are there extreme values (outliers)?\n",
    "        - Do some features have very low variety (eg., same value everywhere)? → not useful.\n",
    "\n",
    "    - **Relationships**\n",
    "        - Which features actually influence the target?\n",
    "        - Feature and feature relationship (Highly correlated features bring redundancy (not useful))\n",
    "\n",
    "    - **Possible enhancements**\n",
    "        - Can we create any valuable new features based on our existing data?\n",
    "        - Are there likely to be enough valuable records to train our model?  \n",
    "\n",
    "    - **Potential issues**\n",
    "        - Missing values\n",
    "        - Outliers\n",
    "        - Wrong data types\n",
    "        - Imbalanced classes (in classification)\n",
    "        - Too few meaningful samples\n",
    "        - Need for encoding categorical features\n",
    "        - Need for scaling numeric features\n",
    "\n",
    "By determining all of this. we complete the EDA stage and thus detect problems early before they ruin our model.\n",
    "\n",
    "\n",
    "#### 2. **Pre-processing:**"
=======
    "### Supervised Learning Flow\n",
    "\n",
    "#### **Phase 1: Training and Testing (Model Creation & Validation)**\n",
    "This phase involves training the algorithm and validating the resulting Statistical Model ($f(x)$) using labeled historical data.\n",
    "\n",
    "| Step | Component | Detailed Role / Percentage |\n",
    "| :--- | :--- | :--- |\n",
    "| 1 | **Historical Data** | The initial, labeled source dataset containing **input features** and their corresponding **known output labels**. |\n",
    "| 2 | **Random Sampling** | The process that randomly divides the Historical Data to create independent sets, ensuring both are representative of the whole. |\n",
    "| 3 | **Training Dataset** | **80%** of the data. This is the portion used by the Machine Learning algorithm to **learn** the underlying patterns and relationships. |\n",
    "| 4 | **Test Dataset** | **20%** of the data. This **hold-out set** is used exclusively to evaluate the model's performance on unseen data before deployment. |\n",
    "| 5 | **Machine Learning** | The computational process where the algorithm iteratively **fits** its parameters to the Training Dataset to minimize errors. |\n",
    "| 6 | **Statistical Model $f(x)$** | The resulting **learned function** (the hypothesis) from the training process, which generalizes the input-output mapping ($y = f(x)$). |\n",
    "| 7 | **Prediction and Testing** | The validated model ($f(x)$) makes predictions on the Test Dataset, which are then compared against the **known labels** to calculate accuracy. |\n",
    "| 8 | **Model Validation Outcome** | The quantitative result (e.g., accuracy score, F1-score) that determines if the model is robust and **fit for use** (meets pre-defined performance metrics). |\n",
    "\n",
    "---\n",
    "\n",
    "####  **Phase 2: Prediction (Deployment)**\n",
    "This phase involves using the validated model ($f(x)$) to generate output for new, unlabeled production data.\n",
    "\n",
    "| Component | Function in the Prediction Phase |\n",
    "| :--- | :--- |\n",
    "| **New Data** | Unseen, unlabeled production data that is fed into the deployed model to obtain a real-time output or forecast. |\n",
    "| **Model ($f(x)$)** | The validated Statistical Model from Phase 1, which now serves as the **prediction engine** in the production environment. |\n",
    "| **Prediction Outcome** | The final, calculated output (label or value) generated by the model for the New Data. |\n",
    "| **Improvement Note** | Prediction accuracy can be enhanced by **more training data**, increasing model **capacity** (complexity), or **algorithm redesign**. |"
>>>>>>> 68457712191c7b29c24e46a702834c7c72225d1d
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "fe4f1591",
=======
   "id": "266b64e6",
>>>>>>> 68457712191c7b29c24e46a702834c7c72225d1d
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
